{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8ddc7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc2749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. Comments on homework\n",
    "2. Theory part\n",
    "    1. Matrix calculus basics (3d task in test 1)\n",
    "    2. Convex differentiable functions (1st task in test 1) \n",
    "    3. GD in convex setup\n",
    "<!--     4. Strong convexity -->\n",
    "2. kinda Real-world task: total variation denoising + in-painting\n",
    "3. Comments on projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50fef7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comments on hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab3fdd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# by Nikita Akshaev\n",
    "def counter(f):\n",
    "    def wrapped(*args):\n",
    "        wrapped.calls+=1\n",
    "        return f(*args)\n",
    "    wrapped.calls = 0\n",
    "    return wrapped\n",
    "\n",
    "@counter\n",
    "def f(x):\n",
    "    return x**6 - x**5 + x**4 - 3*x**3 + x**2 + 2.5*x - 0.1*sin(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007b0e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "В золотом сечении тоже можно было оптимизировать количество вызовов функции, переиспользуя прошлые вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08dd2b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Оформляйте графики!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a89449",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1,1))\n",
    "x, y = [],[]\n",
    "plt.xlabel('xlabel')\n",
    "plt.ylabel('ylabel')\n",
    "plt.title('title')\n",
    "plt.plot(x, y, label='gold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a215d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Theory part\n",
    "## Matrix calculus\n",
    "\n",
    "https://fmin.xyz/docs/theory/Matrix_calculus/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca88a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Basic approach \n",
    "![image](matrix_calculus.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f1b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\nabla$ is just a vector of partial derivatives, we can use all properites of derivates (derivative of sum, product, superposition, etc)\n",
    "\n",
    "Remember vector fields from calculus ($rot$, $div$, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b77f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Just one example (Task 3 from test 1)\n",
    "$$\\| Ax \\|^2 = \\langle Ax, Ax \\rangle = \\langle x,A^\\top Ax \\rangle $$ \n",
    "\n",
    "(we used $\\langle a, b \\rangle = a^\\top b$ in the last equality)\n",
    "\n",
    "$$\\nabla_x \\| Ax \\|^2 = \\nabla_x \\langle Ax, Ax \\rangle = \\nabla_x \\langle A\\overset{\\downarrow}{x}, Ax \\rangle +\n",
    " \\nabla_x \\langle Ax, A\\overset{\\downarrow}{x} \\rangle = 2 \\nabla_x \\langle \\overset{\\downarrow}{x}, A^\\top A x\\rangle  = 2 A^\\top A x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884f3d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differentiable convex functions\n",
    "\n",
    "$$ f(y) \\geq f(x) + \\langle \\nabla f(x), y-x \\rangle $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0e79b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Other properties of differentiable convex and smooth functions\n",
    "(Nesterov)\n",
    "![img](convex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39d67b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent in convex setup\n",
    "\n",
    "Nesterov 2.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93fc96",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In L-smooth setup we obtained \n",
    "\n",
    "$$f(x^{k+1}) - f(x^k) \\leq -w \\|f'(x^k)\\|^2,~~ w = h (1 - Lh/2) $$\n",
    "\n",
    "Denote $\\Delta_k = f(x^k) - f(x^*)$\n",
    "\n",
    "Convexity : $\\Delta_k \\leq \\langle f'(x^k), x^* - x^k \\rangle \\leq \\|f'(x^k)\\| r_0$\n",
    "- We could also use it as a stopping criteria\n",
    "\n",
    "Then $$\\Delta_{k+1} \\leq \\Delta_k - w \\frac{\\Delta_k^2 }{ r_0^2} ~~~| \\cdot \\frac{1}{\\Delta_{k+1}\\Delta_{k} } $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4846ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Telescopic inequality\n",
    "\n",
    "...\n",
    "\n",
    "$\\Delta_0 \\leq \\frac{L}{2}r_0^2,~~ w = \\frac{1}{2L}$\n",
    "\n",
    "$$f(x^k) - f^* \\leq \\frac{2L \\|x^0 - x^*\\|}{k+4}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c130b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775378f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from scipy import misc\n",
    "# enot = (misc.face(gray = True)/255)[:, -768:]\n",
    "\n",
    "mox = plt.imread('mox.jpg')[::3, ::3, :] / 255\n",
    "image = mox\n",
    "\n",
    "plt.imshow(image)\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd0a82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Denoising problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ffa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "x0 = np.clip(image +   0.6*(np.random.random(image.shape) - 0.5), 0, 1) \n",
    "\n",
    "plt.imshow(x0)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308939b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to construct an optimization problem\n",
    "\n",
    "We want to reduce the noise, while keeping resulting image close to the inital noisy picture \n",
    "\n",
    "General approach:  $$f(y) = \\phi(y) + \\|y - x \\|^2_2 \\rightarrow \\min_y$$\n",
    "\n",
    "How to choose the smoothing objective $\\phi(x)$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d578e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Total Variation\n",
    "$l_1$-norm of difference matrix:\n",
    "$$\n",
    "\\phi(X) = TV(X) =  \\sum_{i=1}^{m-1}\\sum_{j=1}^{n-1} \\left\\|\\left[ \\begin{array}{} \n",
    "X_{i+1,j} - X_{ij} \\\\ X_{i, j+1} - X_{ij}\\end{array} \\right] \\right\\|_2\\\\\n",
    "$$\n",
    "\n",
    "- the norms are not squared!\n",
    "- in 1-D case its just $\\sum^{n-1}_1 | x_{i+1} - x_i |$:\n",
    "  if we denote \n",
    "$$D = \\begin{pmatrix}\n",
    "1, -1, 0, 0, \\ldots,0\\\\\n",
    "0, 1, -1, 0, \\ldots, 0\\\\\n",
    " \\ldots\\\\\n",
    "0,0,\\ldots, 0, 1, -1\\\\\n",
    "\\end{pmatrix},$$\n",
    "then TV will be $TV(x) = \\| Dx \\|_1$, not $TV(x) = \\|Dx \\|_2$ !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce485510",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $l_1$-norm or $l_2$-norm \n",
    "(Boyd, Convex Optimization, 6.3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483d551",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![image](l1_smooth.png) -->\n",
    "<img src=\"l1_smooth.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14047b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"l2_smooth.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4e150",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TV is not smooth!\n",
    "\n",
    "We will use a quick engineering hack to smooth the objective \n",
    "$$\\|\\cdot\\|_2 \\rightarrow \\|\\cdot\\|_{2, \\varepsilon} = \\sqrt{\\sum x_i^2 + \\varepsilon}$$\n",
    "\n",
    "More on nonsmooth optimization later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08085b2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "smooth_eps = 1e-3\n",
    "plt.figure(figsize=(14,4))\n",
    "x = np.linspace(-1e-1, 1e-1, 100)\n",
    "plt.subplot(121)\n",
    "plt.plot(x, (x**2 + smooth_eps)**0.5, label='smoothed $l_2$ norm')\n",
    "plt.plot(x, (x**2)**0.5, label='$l_2$ norm')\n",
    "plt.title('$l_2$ norm smoothing')\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(x, x / (x**2 + smooth_eps)**0.5)\n",
    "# plt.title('derivative of smoothed norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563783b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to implement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5d8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Wrong approach - in-python loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8957f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = image\n",
    "m, n, k = x.shape\n",
    "tv = 0\n",
    "for i in range(1, m):\n",
    "    for j in range(1, n):\n",
    "        tv += ((x[i, j] - x[i, j-1])**2 + (x[i, j] - x[i-1, j])**2)**0.5\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9667e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correct approach - using numpy array framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TVsq_matrix(x):\n",
    "    # your code\n",
    "\n",
    "def TV(x):\n",
    "    # your code\n",
    "\n",
    "def dTV(x):\n",
    "    # your code\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.calls = 0\n",
    "        self.log = []\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        self.calls += 1\n",
    "        val = self.f(*args)\n",
    "        self.log.append(np.linalg.norm(val))\n",
    "        return val \n",
    "    \n",
    "    def reset(self):\n",
    "        self.calls = 0\n",
    "        self.log = []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "\n",
    "def f(x, x0):\n",
    "    return TV(x) + alpha * np.linalg.norm(x - x0)\n",
    "    \n",
    "def df(x, x0):\n",
    "    return dTV(x) + alpha * 2 * (x - x0)\n",
    "\n",
    "lf, ldf = Logger(f), Logger(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(f, df, h, x0, niter=300, alpha=0.3, rho=0.8):\n",
    "    # your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ec94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.reset(), ldf.reset()\n",
    "grad = lambda x : ldf(x, x0)\n",
    "fun = lambda x : lf(x, x0)\n",
    "x = gd(fun, grad, 0.1, x0, niter=300)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(x, cmap='gray')\n",
    "# print(np.linalg.norm(df(x, x0)), f(x, x0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "flog_ = np.array(lf.log)\n",
    "flog = [flog_[0]]\n",
    "for f in flog_[1:]:\n",
    "    if f < flog[-1]:\n",
    "        flog.append(f)\n",
    "x = np.arange(1, len(flog) +1 )\n",
    "yref = (flog[0] - flog[-1])/x**1.0 + flog[-1]\n",
    "q = 1\n",
    "def scale(y):\n",
    "    return y\n",
    "plt.plot(scale(flog), label='$f_k$')\n",
    "plt.plot(scale(yref), label='$(f_0 - f_*)/k$')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('f')\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(ldf.log, label='$||df||$')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('$||df||$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(228)\n",
    "# smooth_eps = 1e0\n",
    "corrupted_mask = np.random.random(image.shape[:2]) < 0.98\n",
    "corrupted_mask = np.expand_dims(corrupted_mask, 2)\n",
    "                \n",
    "x0_paint = image * (np.logical_not(corrupted_mask))# + 0.4 * np.random.random(image.shape) * corrupted_mask\n",
    "\n",
    "\n",
    "def df_paint(x):\n",
    "    return dTV(x) * corrupted_mask\n",
    "\n",
    "lf_paint = Logger(TV)\n",
    "ldf_paint = Logger(df_paint)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(x0_paint)\n",
    "\n",
    "painted = gd(lf_paint, ldf_paint, 0.1, x0_paint, 3000)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(painted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db41a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "flog_ = np.array(lf_paint.log)\n",
    "flog = [flog_[0]]\n",
    "for f in flog_[1:]:\n",
    "    if f < flog[-1]:\n",
    "        flog.append(f)\n",
    "flog = np.array(flog)\n",
    "# flog = np.array(lf_paint.log)#[:100]\n",
    "x = np.arange(1, len(flog) +1 )\n",
    "yref = 150*(flog[0] - flog[-1])/(x+4) + flog[-1]\n",
    "\n",
    "q = 1\n",
    "def scale(y):\n",
    "    return (y ** (q))#[-len(y)//10:]\n",
    "plt.plot(scale(flog), label='f')\n",
    "# plt.plot(scale(yref)[100:], label='ref')\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(ldf_paint.log, label='df')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a14bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
